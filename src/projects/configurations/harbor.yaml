# debug, info, warning, error or fatal
logLevel: info

expose:
  # Set how to expose the service. Set the type as "ingress", "clusterIP", "nodePort" or "loadBalancer"
  # and fill the information in the corresponding section
  type: ingress
  # Enable TLS or not.
  # Delete the "ssl-redirect" annotations in "expose.ingress.annotations" when TLS is disabled and "expose.type" is "ingress"
  # Note: if the "expose.type" is "ingress" and TLS is disabled,
  # the port must be included in the command when pulling/pushing images.
  # Refer to https://github.com/goharbor/harbor/issues/5291 for details.
  tls:
    enabled: true
    # The source of the tls certificate. Set as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: secret
    secret:
      # The name of secret which contains keys named:
      # "tls.crt" - the certificate
      # "tls.key" - the private key
      secretName: "helm-harbor-ingress"
  ingress:
    annotations:
      kubernetes.io/ingress.class: traefik
      cert-manager.io/cluster-issuer: letsencrypt-prd
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      # note different ingress controllers may require a different ssl-redirect annotation
      # for Envoy, use ingress.kubernetes.io/force-ssl-redirect: "true" and remove the nginx lines below
    className: "traefik"
    hosts:
      core: ocr.jcan.dev
      notary: notary.jcan.dev

# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
# 2) if "expose.type" is "clusterIP", the "domain" should be
# the value of "expose.clusterIP.name"
# 3) if "expose.type" is "nodePort", the "domain" should be
# the IP address of k8s node
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: https://ocr.jcan.dev

controller:
  resources:
    requests:
      cpu: 2000m
      memory: 2Gi
    limits:
      cpu: 4000m
      memory: 4Gi

core:
  resources:
    requests:
      cpu: 2000m
      memory: 2Gi
    limits:
      cpu: 4000m
      memory: 4Gi
  extraEnvVars: []
  #   - name: CONFIG_OVERWRITE_JSON
  #     valueFrom:
  #       secretKeyRef:
  #         name: harbor-config
  #         key: CONFIG_OVERWRITE_JSON

database:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/etcd
                operator: DoesNotExist
  internal:
    shmSizeLimit: 4Gi
    resources:
      requests:
        memory: 2Gi
        cpu: 2000m
      limits:
        memory: 4Gi
        cpu: 4000m

    livenessProbe:
      enabled: true
      initialDelaySeconds: 300
      periodSeconds: 100
      timeoutSeconds: 50
      successThreshold: 10
      failureThreshold: 60
      
    readinessProbe:
      enabled: true
      initialDelaySeconds: 50
      periodSeconds: 100
      timeoutSeconds: 50
      successThreshold: 10
      failureThreshold: 60
      
    startupProbe:
      enabled: false
      initialDelaySeconds: 50
      periodSeconds: 100
      timeoutSeconds: 50
      successThreshold: 10
      failureThreshold: 100

jobservice:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/etcd
                operator: DoesNotExist

portal:
  resources:
    requests:
      cpu: 2000m
      memory: 2Gi

registry:
  resources:
    requests:
      memory: 4Gi
      cpu: 1000m
  affinity: {}

# The persistence is enabled by default and a default StorageClass
# is needed in the k8s cluster to provision volumes dynamically.
# Specify another StorageClass in the "storageClass" or set "existingClaim"
# if you already have existing persistent volumes to use
#
# For storing images and charts, you can also use "azure", "gcs", "s3",
# "swift" or "oss". Set it in the "imageChartStorage" section
persistence:
  enabled: true
  # Setting it to "keep" to avoid removing PVCs during a helm delete
  # operation. Leaving it empty will delete PVCs after the chart deleted
  # (this does not apply for PVCs that are created for internal database
  # and redis components, i.e. they are never deleted automatically)
  resourcePolicy: "keep"
  persistentVolumeClaim:
    registry:
      storageClass: "longhorn"
      accessMode: ReadWriteMany
      size: 100Gi
      annotations: {}
    jobservice:
      jobLog:
        storageClass: "longhorn"
        accessMode: ReadWriteMany
        size: 5Gi
        annotations: {}
    database:
      storageClass: "longhorn"
      accessMode: ReadWriteMany
      size: 10Gi
      annotations: {}
    redis:
      storageClass: "longhorn"
      subPath: ""
      accessMode: ReadWriteMany
      size: 5Gi
      annotations: {}
    trivy:
      storageClass: "longhorn"
      accessMode: ReadWriteMany
      size: 15Gi
      annotations: {}

trivy:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/etcd
                operator: DoesNotExist
  timeout: 30m
  resources:
    requests:
      cpu: 2
      memory: 2Gi
    limits:
      cpu: 4
      memory: 4Gi
