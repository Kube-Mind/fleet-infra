# -- Overrides the chart's computed fullname
fullnameOverride: "loki"
# -- Deployment mode lets you specify how to deploy Loki.
# There are 3 options:
# - SingleBinary: Loki is deployed as a single binary, useful for small installs typically without HA, up to a few tens of GB/day.
# - SimpleScalable: Loki is deployed as 3 targets: read, write, and backend. Useful for medium installs easier to manage than distributed, up to a about 1TB/day.
# - Distributed: Loki is deployed as individual microservices. The most complicated but most capable, useful for large installs, typically over 1TB/day.
# There are also 2 additional modes used for migrating between deployment modes:
# - SingleBinary<->SimpleScalable: Migrate from SingleBinary to SimpleScalable (or vice versa)
# - SimpleScalable<->Distributed: Migrate from SimpleScalable to Distributed (or vice versa)
# Note: SimpleScalable and Distributed REQUIRE the use of object storage.
deploymentMode: SimpleScalable
######################################################################################################################
#
# Base Loki Configs including kubernetes configurations and configurations for Loki itself,
# see below for more specifics on Loki's configuration.
#
######################################################################################################################
# -- Configuration for running Loki
# @default -- See values.yaml
loki:
  # Configures the liveness probe for all of the Loki pods
  livenessProbe: {}
  # Configures the readiness probe for all of the Loki pods
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    periodSeconds: 10
    initialDelaySeconds: 15
    successThreshold: 1
    failureThreshold: 3
    timeoutSeconds: 1
  # Configures the startup probe for all of the Loki pods
  startupProbe: {}
  image:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: grafana/loki
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: 3.6.5
    # -- Overrides the image tag with an image digest
    digest: null
    # -- Docker image pull policy
    pullPolicy: IfNotPresent
  # -- Common annotations for all deployments/StatefulSets
  annotations: {}
  # -- Common annotations for all pods
  podAnnotations: {}
  # -- Common labels for all pods
  podLabels: {}
  # -- Common annotations for all services
  serviceAnnotations: {}
  # -- Common labels for all services
  serviceLabels: {}
  # -- The number of old ReplicaSets to retain to allow rollback
  revisionHistoryLimit: 10
  # -- The SecurityContext for Loki pods
  podSecurityContext:
    fsGroup: 10001
    fsGroupChangePolicy: OnRootMismatch
    runAsGroup: 10001
    runAsNonRoot: true
    runAsUser: 10001
  # -- The SecurityContext for Loki containers
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false
  # -- Should enableServiceLinks be enabled. Default to enable
  enableServiceLinks: true
  # -- DNS config for Loki pods
  dnsConfig: {}
  ######################################################################################################################
  #
  # Loki Configuration
  #
  # There are several ways to pass configuration to Loki, listing them here in order of our preference for how
  # you should use this chart.
  # 1. Use the templated value of loki.config below and the corresponding override sections which follow.
  #    This allows us to set a lot of important Loki configurations and defaults and also allows us to maintain them
  #    over time as Loki changes and evolves.
  # 2. Use the loki.structuredConfig section.
  #    This will completely override the templated value of loki.config, so you MUST provide the entire Loki config
  #    including any configuration that we set in loki.config unless you explicitly are trying to change one of those
  #    values and are not able to do so with the templated sections.
  #    If you choose this approach the burden is on you to maintain any changes we make to the templated config.
  # 3. Use an existing secret or configmap to provide the configuration.
  #    This option is mostly provided for folks who have external processes which provide or modify the configuration.
  #    When using this option you can specify a different name for loki.generatedConfigObjectName and configObjectName
  #    if you have a process which takes the generated config and modifies it, or you can stop the chart from generating
  #    a config entirely by setting loki.generatedConfigObjectName to
  #
  ######################################################################################################################

  # Should authentication be enabled
  auth_enabled: true
  # -- memberlist configuration (overrides embedded default)
  memberlistConfig: {}
  # -- Extra memberlist configuration
  extraMemberlistConfig: {}
  # -- Tenants list to be created on nginx htpasswd file, with name and password or passwordHash keys<br><br>
  # Example:
  # <pre>
  # tenants:<br>
  #   - name: "test-user-1"<br>
  #     password: "test-password-1"<br>
  #   - name: "test-user-2"<br>
  #     passwordHash: "$2y$10$7O40CaY1yz7fu9O24k2/u.ct/wELYHRBsn25v/7AyuQ8E8hrLqpva" # generated using `htpasswd -nbBC10 test-user-2 test-password-2`
  # </pre>
  tenants: []
  # -- Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.
  server:
    http_listen_port: 3100
    grpc_listen_port: 9095
    http_server_read_timeout: 600s
    http_server_write_timeout: 600s
  service:
    # -- trafficDistribution for services
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#traffic-distribution
    trafficDistribution: ""
  # -- Limits config
  limits_config:
    reject_old_samples: true
    reject_old_samples_max_age: 168h
    max_cache_freshness_per_query: 10m
    split_queries_by_interval: 15m
    query_timeout: 300s
    volume_enabled: true
  # -- Provides a reloadable runtime configuration file for some specific configuration
  runtimeConfig: {}
  # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration
  commonConfig:
    path_prefix: /var/loki
    replication_factor: 3
    # -- The gRPC address of the compactor. The use of compactor_grpc_address is prefered over compactor_address.
    # If a customized compactor_address is set, compactor_grpc_address should be set to an empty string.
    compactor_grpc_address: '{{ include "loki.compactorAddress" . }}'
  analytics: {}
  # --  Optional Loki UI: Provides access to a operators UI for Loki distributed. When enabled UI will be available at /ui/ of loki-gateway
  ui:
    # Disabled by default for backwards compatibility. Enable to use the Loki UI.
    enabled: false
    gateway:
      # enable gateway proxying to UI under /ui
      enabled: true
  # --  Optional querier configuration
  query_range: {}
  # --  Optional querier configuration
  querier: {}
  # --  Optional ingester configuration
  ingester: {}
  # --  Optional ingester client configuration
  ingester_client: {}
  # --  Optional block builder configuration
  block_builder: {}
  # --  Optional index gateway configuration
  index_gateway:
    mode: simple
  frontend:
    scheduler_address: '{{ include "loki.querySchedulerAddress" . }}'
    tail_proxy_url: '{{ include "loki.querierAddress" . }}'
  frontend_worker:
    scheduler_address: '{{ include "loki.querySchedulerAddress" . }}'
  # -- Optional distributor configuration
  distributor: {}
  # -- Enable tracing
  tracing:
    enabled: false
  bloom_build:
    enabled: false
    builder:
      planner_address: '{{ include "loki.bloomPlannerAddress" . }}'
  bloom_gateway:
    enabled: false
    client:
      addresses: '{{ include "loki.bloomGatewayAddresses" . }}'
  # -- Optional operational configuration
  operational_config: {}

######################################################################################################################
#
# Chart Testing
#
######################################################################################################################

# -- Section for configuring optional Helm test
test:
  enabled: true
  # -- Used to directly query the metrics endpoint of the canary for testing, this approach avoids needing prometheus for testing.
  # This in a newer approach to using prometheusAddress such that tests do not have a dependency on prometheus
  canaryServiceAddress: 'http://{{ include "loki-canary.fullname" $ }}.{{ include "loki.namespace" $ }}.svc.{{ .Values.global.clusterDomain }}:3500/metrics'
  # -- Address of the prometheus server to query for the test. This overrides any value set for canaryServiceAddress.
  # This is kept for backward compatibility and may be removed in future releases. Previous value was 'http://prometheus:9090'
  prometheusAddress: ""
  # -- Number of times to retry the test before failing
  timeout: 1m
  # -- Additional labels for the test pods
  labels: {}
  # -- Additional annotations for test pods
  annotations: {}
  # -- Image to use for loki canary
  image:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: grafana/loki-helm-test
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: "latest"
    # -- Overrides the image tag with an image digest
    digest: null
    # -- Docker image pull policy
    pullPolicy: IfNotPresent
  # -- Use the host's user namespace in test pods
  hostUsers: nil
# The Loki canary pushes logs to and queries from this loki installation to test
# that it's working correctly
lokiCanary:
  enabled: true
  # -- The type of the loki canary k8s rollout. This can be a DaemonSet or Deployment.
  kind: DaemonSet
  # -- If true, the canary will send directly to Loki via the address configured for verification --
  # -- If false, it will write to stdout and an Agent will be needed to scrape and send the logs --
  push: true
  # -- If set overwrites the default value set by loki.host helper function. Use this if gateway not enabled.
  lokiurl: null
  # -- The name of the label to look for at loki when doing the checks.
  labelname: pod
  # -- Additional annotations for the `loki-canary` Daemonset
  annotations: {}
  # -- Additional labels for each `loki-canary` pod
  podLabels: {}
  service:
    # -- Annotations for loki-canary Service
    annotations: {}
    # -- Additional labels for loki-canary Service
    labels: {}
  # -- Additional CLI arguments for the `loki-canary' command
  extraArgs: []
  # -- Environment variables to add to the canary pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the canary pods
  extraEnvFrom: []
  # -- Volume mounts to add to the canary pods
  extraVolumeMounts: []
  # -- Volumes to add to the canary pods
  extraVolumes: []
  # -- Resource requests and limits for the canary
  resources: {}
  # -- DNS config for canary pods
  dnsConfig: {}
  # -- Node selector for canary pods
  nodeSelector: {}
  # -- Tolerations for canary pods
  tolerations: []
  # -- Affinity for canary pods
  affinity: {}
  # -- The name of the PriorityClass for loki-canary pods
  priorityClassName: null
  # -- Use the host's user namespace in loki-canary pods
  hostUsers: nil
  # -- Image to use for loki canary
  image:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: grafana/loki-canary
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: null
    # -- Overrides the image tag with an image digest
    digest: null
    # -- Docker image pull policy
    pullPolicy: IfNotPresent
  # -- Liveness probe
  livenessProbe:
  # -- Readiness probe
  readinessProbe:
    httpGet:
      path: /metrics
      port: http-metrics
    initialDelaySeconds: 15
    timeoutSeconds: 1
  # -- Startup probe
  startupProbe:
  # -- Update strategy for the `loki-canary` Daemonset pods
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  # -- Replicas for `loki-canary` when using a Deployment
  replicas: 1
######################################################################################################################
#
# Service Accounts and Kubernetes RBAC
#
######################################################################################################################
serviceAccount:
  # -- Specifies whether a ServiceAccount should be created
  create: true
  # -- The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name: null
  # -- Image pull secrets for the service account
  imagePullSecrets: []
  # -- Annotations for the service account
  annotations: {}
  # -- Labels for the service account
  labels: {}
  # -- Set this toggle to false to opt out of automounting API credentials for the service account
  automountServiceAccountToken: true
# RBAC configuration
rbac:
  # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.
  pspEnabled: false
  # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.
  sccEnabled: false
  # -- Toggle this to true to allow the use of hostPath volumes on OpenShift
  sccAllowHostDirVolumePlugin: false
  # -- Specify PSP annotations
  # Ref: https://kubernetes.io/docs/reference/access-authn-authz/psp-to-pod-security-standards/#podsecuritypolicy-annotations
  pspAnnotations: {}
  # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
  # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
  # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
  # -- Whether to install RBAC in the namespace only or cluster-wide. Useful if you want to watch ConfigMap globally.
  namespaced: false
######################################################################################################################
#
# Network Policy configuration
#
######################################################################################################################
networkPolicy:
  # -- Specifies whether Network Policies should be created
  enabled: false
  # -- Specifies whether the policies created will be standard Network Policies (flavor: kubernetes)
  # or Cilium Network Policies (flavor: cilium)
  flavor: kubernetes
  metrics:
    # -- Specifies the Pods which are allowed to access the metrics port.
    # As this is cross-namespace communication, you also need the namespaceSelector.
    podSelector: {}
    # -- Specifies the namespaces which are allowed to access the metrics port
    namespaceSelector: {}
    # -- Specifies specific network CIDRs which are allowed to access the metrics port.
    # In case you use namespaceSelector, you also have to specify your kubelet networks here.
    # The metrics ports are also used for probes.
    cidrs: []
  ingress:
    # -- Specifies the Pods which are allowed to access the http port.
    # As this is cross-namespace communication, you also need the namespaceSelector.
    podSelector: {}
    # -- Specifies the namespaces which are allowed to access the http port
    namespaceSelector: {}
  alertmanager:
    # -- Specify the alertmanager port used for alerting
    port: 9093
    # -- Specifies the alertmanager Pods.
    # As this is cross-namespace communication, you also need the namespaceSelector.
    podSelector: {}
    # -- Specifies the namespace the alertmanager is running in
    namespaceSelector: {}
  externalStorage:
    # -- Specify the port used for external storage, e.g. AWS S3
    ports: []
    # -- Specifies specific network CIDRs you want to limit access to
    cidrs: []
  discovery:
    # -- (int) Specify the port used for discovery
    port: null
    # -- Specifies the Pods labels used for discovery.
    # As this is cross-namespace communication, you also need the namespaceSelector.
    podSelector: {}
    # -- Specifies the namespace the discovery Pods are running in
    namespaceSelector: {}
  egressWorld:
    # -- Enable additional cilium egress rules to external world for write, read and backend.
    enabled: false
  egressKubeApiserver:
    # -- Enable additional cilium egress rules to kube-apiserver for backend.
    enabled: false
######################################################################################################################
#
# Global memberlist configuration
#
######################################################################################################################

# Configuration for the memberlist service
memberlist:
  service:
    publishNotReadyAddresses: false
    annotations: {}
######################################################################################################################
#
# adminAPI configuration, enterprise only.
#
######################################################################################################################

# -- Configuration for the `admin-api` target
adminApi:
  # -- Define the amount of instances
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Additional CLI arguments for the `admin-api` target
  extraArgs: {}
  # -- Environment variables to add to the admin-api pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the admin-api pods
  extraEnvFrom: []
  # -- Additional labels for the `admin-api` Deployment
  labels: {}
  # -- Additional annotations for the `admin-api` Deployment
  annotations: {}
  # -- DNSConfig for `admin-api` pods
  dnsConfig: {}
  # -- Additional labels and annotations for the `admin-api` Service
  service:
    labels: {}
    annotations: {}
  # -- Run container as user `enterprise-logs(uid=10001)`
  # `fsGroup` must not be specified, because these security options are applied
  # on container level not on Pod level.
  podSecurityContext:
    runAsNonRoot: true
    runAsGroup: 10001
    runAsUser: 10001
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false
  # -- Update strategy
  strategy:
    type: RollingUpdate
  # -- Liveness probe
  livenessProbe: {}
  # -- Readiness probe
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  # -- Startup probe
  startupProbe: {}
  # -- Request and limit Kubernetes resources
  # -- Values are defined in small.yaml and large.yaml
  resources: {}
  # -- Configure optional environment variables
  env: []
  # -- Configure optional initContainers
  initContainers: []
  # -- Configure optional extraContainers
  extraContainers: []
  # -- Additional volumes for Pods
  extraVolumes: []
  # -- Additional volume mounts for Pods
  extraVolumeMounts: []
  # -- Affinity for admin-api Pods
  # The value will be passed through tpl.
  affinity: {}
  # -- Node selector for admin-api Pods
  nodeSelector: {}
  # -- Topology Spread Constraints for admin-api pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for admin-api Pods
  tolerations: []
  # -- Grace period to allow the admin-api to shutdown before it is killed
  terminationGracePeriodSeconds: 60
  # -- Use the host's user namespace in admin-api pods
  hostUsers: nil
######################################################################################################################
#
# Gateway and Ingress
#
# By default this chart will deploy a Nginx container to act as a gateway which handles routing of traffic
# and can also do auth.
#
# If you would prefer you can optionally disable this and enable using k8s ingress to do the incoming routing.
#
######################################################################################################################

# Configuration for the gateway
gateway:
  # -- Specifies whether the gateway should be enabled
  enabled: true
  # -- Number of replicas for the gateway
  replicas: 1
  # -- Default container port
  containerPort: 8080
  # -- Enable logging of 2xx and 3xx HTTP requests
  verboseLogging: true
  autoscaling:
    # -- Enable autoscaling for the gateway
    enabled: false
    # -- Minimum autoscaling replicas for the gateway
    minReplicas: 1
    # -- Maximum autoscaling replicas for the gateway
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the gateway
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the gateway
    targetMemoryUtilizationPercentage:
    # -- See `kubectl explain deployment.spec.strategy` for more
    # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
    # -- Behavior policies while scaling.
    behavior: {}
    #    scaleUp:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 60
    #    scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 180
  deploymentStrategy:
    type: RollingUpdate
  image:
    # -- The Docker registry for the gateway image
    registry: docker.io
    # -- The gateway image repository
    repository: nginxinc/nginx-unprivileged
    # -- The gateway image tag
    tag: 1.29-alpine
    # -- Overrides the gateway image tag with an image digest
    digest: null
    # -- The gateway image pull policy
    pullPolicy: IfNotPresent
  # -- The name of the PriorityClass for gateway pods
  priorityClassName: null
  # -- Annotations for gateway deployment
  annotations: {}
  # -- Annotations for gateway pods
  podAnnotations: {}
  # -- Additional labels for gateway pods
  podLabels: {}
  # -- Additional CLI args for the gateway
  extraArgs: []
  # -- Environment variables to add to the gateway pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the gateway pods
  extraEnvFrom: []
  # -- Lifecycle for the gateway container
  lifecycle: {}
  # -- Volumes to add to the gateway pods
  extraVolumes: []
  # -- Volume mounts to add to the gateway pods
  extraVolumeMounts: []
  # -- The SecurityContext for gateway containers
  podSecurityContext:
    fsGroup: 101
    runAsGroup: 101
    runAsNonRoot: true
    runAsUser: 101
  # -- The SecurityContext for gateway containers
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false
  # -- Use the host's user namespace in the gateway
  hostUsers: nil
  # -- Resource requests and limits for the gateway
  resources: {}
  # -- Containers to add to the gateway pods
  extraContainers: []
  # -- Grace period to allow the gateway to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for gateway pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: gateway
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- DNS config for gateway pods
  dnsConfig: {}
  # -- Node selector for gateway pods
  nodeSelector: {}
  # -- Topology Spread Constraints for gateway pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for gateway pods
  tolerations: []
  # Gateway service configuration
  service:
    # -- Port of the gateway service
    port: 80
    # -- Type of the gateway service
    type: ClusterIP
    # -- ClusterIP of the gateway service
    clusterIP: null
    # -- (int) Node port if service type is NodePort
    nodePort: null
    # -- Load balancer IPO address if service type is LoadBalancer
    loadBalancerIP: null
    # -- Annotations for the gateway service
    annotations: {}
    # -- Labels for gateway service
    labels: {}
    # -- trafficDistribution for gateway service
    trafficDistribution: ""
  # Gateway ingress configuration
  ingress:
    # -- Specifies whether an ingress for the gateway should be created
    enabled: false
    # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
    ingressClassName: ""
    # -- Annotations for the gateway ingress
    annotations: {}
    # -- Labels for the gateway ingress
    labels: {}
    # -- Hosts configuration for the gateway ingress, passed through the `tpl` function to allow templating
    hosts:
      - host: gateway.loki.example.com
        paths:
          - path: /
            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
            # pathType: Prefix
    # -- TLS configuration for the gateway ingress. Hosts passed through the `tpl` function to allow templating
    tls:
      - secretName: loki-gateway-tls
        hosts:
          - gateway.loki.example.com
  # Basic auth configuration
  basicAuth:
    # -- Enables basic authentication for the gateway
    enabled: false
    # -- The basic auth username for the gateway
    username: null
    # -- The basic auth password for the gateway
    password: null
    # -- Uses the specified users from the `loki.tenants` list to create the htpasswd file.
    # if `loki.tenants` is not set, the `gateway.basicAuth.username` and `gateway.basicAuth.password` are used.
    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes
    # high CPU load.
    # @default -- Either `loki.tenants` or `gateway.basicAuth.username` and `gateway.basicAuth.password`.
    htpasswd: |
      {{- with $tenants := .Values.loki.tenants }}
        {{- range $t := $tenants }}
          {{- $username := required "All tenants must have a 'name' set" $t.name }}
          {{- if $passwordHash := $t.passwordHash }}
            {{- printf "%s:%s\n" $username $passwordHash }}
          {{- else if $password := $t.password }}
            {{- printf "%s\n" (htpasswd $username $password) }}
          {{- else }}
            {{- fail "All tenants must have a 'password' or 'passwordHash' set" }}
          {{- end }}
        {{- end }}
      {{- else }}
        {{- printf "%s\n" (htpasswd (required "'gateway.basicAuth.username' is required" .Values.gateway.basicAuth.username) (required "'gateway.basicAuth.password' is required" .Values.gateway.basicAuth.password)) }}
      {{- end }}
    # -- Existing basic auth secret to use. Must contain '.htpasswd'
    existingSecret: null
  # -- liveness probe for the nginx container in the gateway pods.
  livenessProbe: {}
  # Configures the readiness probe for the gateway
  readinessProbe:
    httpGet:
      path: /
      port: http-metrics
    initialDelaySeconds: 15
    timeoutSeconds: 1
  # -- startup probe for the nginx container in the gateway pods.
  startupProbe: {}
  nginxConfig:
    # -- Which schema to be used when building URLs. Can be 'http' or 'https'.
    schema: http
    # -- Enable listener for IPv6, disable on IPv4-only systems
    enableIPv6: true
    # -- NGINX log format
    logFormat: |-
      main '$remote_addr - $remote_user [$time_local]  $status '
              '"$request" $body_bytes_sent "$http_referer" '
              '"$http_user_agent" "$http_x_forwarded_for"';
    # -- Allows appending custom configuration to the server block
    serverSnippet: ""
    # -- Allows appending custom configuration to the http block, passed through the `tpl` function to allow templating
    httpSnippet: ""
    # -- Allows appending custom configuration inside every location block, useful for authentication or setting headers that are not inherited from the server block, passed through the `tpl` function to allow templating.
    locationSnippet: >-
      {{ if .Values.loki.tenants }}proxy_set_header X-Scope-OrgID $remote_user;{{ end }}
    # -- Allows customizing the `client_max_body_size` directive
    clientMaxBodySize: 4M
    # -- Whether ssl should be appended to the listen directive of the server block or not.
    ssl: false
    # -- Override Read URL
    customReadUrl: null
    # -- Override Write URL
    customWriteUrl: null
    # -- Override Backend URL
    customBackendUrl: null
    # -- Allows overriding the DNS resolver address nginx will use.
    resolver: ""
    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating
    # @default -- See values.yaml
    file: |
      {{- include "loki.nginxFile" . -}}
# -- If running enterprise and using the default enterprise gateway, configs go here.
enterpriseGateway:
  # -- Define the amount of instances
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Use the host's user namespace in the `gateway` pod
  hostUsers: nil
  # -- Additional CLI arguments for the `gateway` target
  extraArgs: {}
  # -- Environment variables from secrets or configmaps to add to the enterprise gateway pods
  extraEnvFrom: []
  # -- Additional labels for the `gateway` Pod
  labels: {}
  # -- Additional annotations for the `gateway` Pod
  annotations: {}
  # -- Additional labels and annotations for the `gateway` Service
  # -- Service overriding service type
  service:
    type: ClusterIP
    labels: {}
    annotations: {}
  # -- Run container as user `enterprise-logs(uid=10001)`
  podSecurityContext:
    runAsNonRoot: true
    runAsGroup: 10001
    runAsUser: 10001
    fsGroup: 10001
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false
  # -- If you want to use your own proxy URLs, set this to false.
  useDefaultProxyURLs: true
  # -- update strategy
  strategy:
    type: RollingUpdate
  # -- Liveness probe
  livenessProbe: {}
  # -- Readiness probe
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  # -- Startup probe
  startupProbe: {}
  # -- Request and limit Kubernetes resources
  # -- Values are defined in small.yaml and large.yaml
  resources: {}
  # -- Configure optional environment variables
  env: []
  # -- Configure optional initContainers
  initContainers: []
  # -- Conifgure optional extraContainers
  extraContainers: []
  # -- Additional volumes for Pods
  extraVolumes: []
  # -- Additional volume mounts for Pods
  extraVolumeMounts: []
  # -- Affinity for gateway Pods
  # The value will be passed through tpl.
  affinity: {}
  # -- Node selector for gateway Pods
  nodeSelector: {}
  # -- Topology Spread Constraints for enterprise-gateway pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for gateway Pods
  tolerations: []
  # -- Grace period to allow the gateway to shutdown before it is killed
  terminationGracePeriodSeconds: 60
# -- Ingress configuration Use either this ingress or the gateway, but not both at once.
# If you enable this, make sure to disable the gateway.
# You'll need to supply authn configuration for your ingress controller.
ingress:
  enabled: false
  ingressClassName: ""
  annotations: {}
  #    nginx.ingress.kubernetes.io/auth-type: basic
  #    nginx.ingress.kubernetes.io/auth-secret: loki-distributed-basic-auth
  #    nginx.ingress.kubernetes.io/auth-secret-type: auth-map
  #    nginx.ingress.kubernetes.io/configuration-snippet: |
  #      proxy_set_header X-Scope-OrgID $remote_user;
  labels: {}
  #    blackbox.monitoring.exclude: "true"
  paths:
    # -- Paths that are exposed by Loki Distributor.
    # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.distributorFullname"}}`.
    # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{"loki.writeFullname"}}`.
    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
    distributor:
      - /api/prom/push
      - /loki/api/v1/push
      - /otlp/v1/logs
      - /ui
    # -- Paths that are exposed by Loki Query Frontend.
    # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.queryFrontendFullname"}}`.
    # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{"loki.readFullname"}}`.
    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
    queryFrontend:
      - /api/prom/query
      # this path covers labels and labelValues endpoints
      - /api/prom/label
      - /api/prom/series
      - /api/prom/tail
      - /loki/api/v1/query
      - /loki/api/v1/query_range
      - /loki/api/v1/tail
      # this path covers labels and labelValues endpoints
      - /loki/api/v1/label
      - /loki/api/v1/labels
      - /loki/api/v1/series
      - /loki/api/v1/index/stats
      - /loki/api/v1/index/volume
      - /loki/api/v1/index/volume_range
      - /loki/api/v1/format_query
      - /loki/api/v1/detected_field
      - /loki/api/v1/detected_fields
      - /loki/api/v1/detected_labels
      - /loki/api/v1/patterns
    # -- Paths that are exposed by Loki Ruler.
    # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.rulerFullname"}}`.
    # If deployment mode is SimpleScalable, the requests are forwarded to k8s service: `{{"loki.backendFullname"}}`.
    # If deployment mode is SimpleScalable but `read.legacyReadTarget` is `true`, the requests are forwarded to k8s service: `{{"loki.readFullname"}}`.
    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
    ruler:
      - /api/prom/rules
      - /api/prom/api/v1/rules
      - /api/prom/api/v1/alerts
      - /loki/api/v1/rules
      - /prometheus/api/v1/rules
      - /prometheus/api/v1/alerts
    # -- Paths that are exposed by Loki Compactor.
    # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.compactorFullname"}}`.
    # If deployment mode is SimpleScalable, the requests are forwarded to k8s service: `{{"loki.backendFullname"}}`.
    # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
    compactor:
      - /loki/api/v1/delete
  # -- Hosts configuration for the ingress, passed through the `tpl` function to allow templating
  hosts:
    - loki.example.com
  # -- TLS configuration for the ingress. Hosts passed through the `tpl` function to allow templating
  tls: []
#    - hosts:
#       - loki.example.com
#      secretName: loki-distributed-tls

######################################################################################################################
#
# Migration
#
######################################################################################################################

# -- Options that may be necessary when performing a migration from another helm chart
migrate:
  # -- When migrating from a distributed chart like loki-distributed or enterprise-logs
  fromDistributed:
    # -- Set to true if migrating from a distributed helm chart
    enabled: false
    # -- If migrating from a distributed service, provide the distributed deployment's
    # memberlist service DNS so the new deployment can join its ring.
    memberlistService: ""
######################################################################################################################
#
# Simple Scalable Deployment (SSD) Mode
#
# For small to medium size Loki deployments up to around 1 TB/day, this is the default mode for this helm chart
#
######################################################################################################################

# Configuration for the write pod(s)
write:
  # -- Number of replicas for the write
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the write.
    enabled: false
    # -- Minimum autoscaling replicas for the write.
    minReplicas: 2
    # -- Maximum autoscaling replicas for the write.
    maxReplicas: 6
    # -- Target CPU utilisation percentage for the write.
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilization percentage for the write.
    targetMemoryUtilizationPercentage:
    # -- Behavior policies while scaling.
    behavior:
      # -- see https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown for scaledown details
      scaleUp:
        policies:
          - type: Pods
            value: 1
            periodSeconds: 900
      scaleDown:
        policies:
          - type: Pods
            value: 1
            periodSeconds: 1800
        stabilizationWindowSeconds: 3600
  image:
    # -- The Docker registry for the write image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the write image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the write image. Overrides `loki.image.tag`
    tag: null
  # -- The name of the PriorityClass for write pods
  priorityClassName: null
  # -- Annotations for write StatefulSet
  annotations: {}
  # -- Annotations for write pods
  podAnnotations: {}
  # -- Additional labels for each `write` pod
  podLabels: {}
  # -- Additional selector labels for each `write` pod
  selectorLabels: {}
  service:
    # -- Annotations for write Service
    annotations: {}
    # -- Additional labels for write Service
    labels: {}
    # -- Service Type for write Service
    type: "ClusterIP"
    # -- trafficDistribution for write service
    trafficDistribution: ""
  # -- Comma-separated list of Loki modules to load for the write
  targetModule: "write"
  # -- Additional CLI args for the write
  extraArgs: []
  # -- Environment variables to add to the write pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the write pods
  extraEnvFrom: []
  # -- Lifecycle for the write container
  lifecycle: {}
  # -- The default /flush_shutdown preStop hook is recommended as part of the ingester
  # scaledown process so it's added to the template by default when autoscaling is enabled,
  # but it's disabled to optimize rolling restarts in instances that will never be scaled
  # down or when using chunks storage with WAL disabled.
  # https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown
  # -- Init containers to add to the write pods
  initContainers: []
  # -- Containers to add to the write pods
  extraContainers: []
  # -- Volume mounts to add to the write pods
  extraVolumeMounts: []
  # -- Volumes to add to the write pods
  extraVolumes: []
  # -- volumeClaimTemplates to add to StatefulSet
  extraVolumeClaimTemplates: []
  # -- Resource requests and limits for the write
  resources: {}
  # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingester,
  # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring
  # all data and to successfully leave the member ring on shutdown.
  terminationGracePeriodSeconds: 300
  # -- Use the host's user namespace in the write pods.
  hostUsers: nil
  # -- Affinity for write pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: write
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- DNS config for write pods
  dnsConfig: {}
  # -- Node selector for write pods
  nodeSelector: {}
  # -- Topology Spread Constraints for write pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for write pods
  tolerations: []
  # -- The default is to deploy all pods in parallel.
  podManagementPolicy: "Parallel"
  persistence:
    # -- Enable volume claims in pod spec
    volumeClaimsEnabled: true
    # -- Set access modes on the PersistentVolumeClaim
    accessModes:
      - ReadWriteOnce
    # -- Parameters used for the `data` volume when volumeClaimEnabled if false
    dataVolumeParameters:
      emptyDir: {}
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: false
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Volume attributes class name to be used.
    # If empty or set to null, no volumeAttributesClassName spec is set.
    # Requires Kubernetes 1.31
    volumeAttributesClassName: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
    # -- Labels for volume claim
    labels: {}
# --  Configuration for the read pod(s)
read:
  # -- Number of replicas for the read
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`
    enabled: false
    # -- Minimum autoscaling replicas for the read
    minReplicas: 2
    # -- Maximum autoscaling replicas for the read
    maxReplicas: 6
    # -- Target CPU utilisation percentage for the read
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the read
    targetMemoryUtilizationPercentage:
    # -- Behavior policies while scaling.
    behavior: {}
    #  scaleUp:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 1
    #     periodSeconds: 60
    #  scaleDown:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 1
    #     periodSeconds: 180
  image:
    # -- The Docker registry for the read image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the read image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the read image. Overrides `loki.image.tag`
    tag: null
  # -- The name of the PriorityClass for read pods
  priorityClassName: null
  # -- Annotations for read deployment
  annotations: {}
  # -- Annotations for read pods
  podAnnotations: {}
  # -- Additional labels for each `read` pod
  podLabels: {}
  # -- Additional selector labels for each `read` pod
  selectorLabels: {}
  service:
    # -- Annotations for read Service
    annotations: {}
    # -- Additional labels for read Service
    labels: {}
    # -- Service Type for read Service
    type: ClusterIP
    # -- trafficDistribution for read service
    trafficDistribution: ""
  # -- Comma-separated list of Loki modules to load for the read
  targetModule: "read"
  # -- Whether or not to use the 2 target type simple scalable mode (read, write) or the
  # 3 target type (read, write, backend). Legacy refers to the 2 target type, so true will
  # run two targets, false will run 3 targets.
  legacyReadTarget: false
  # -- Additional CLI args for the read
  extraArgs: []
  # -- init containers to add to the read pods
  initContainers: []
  # -- Containers to add to the read pods
  extraContainers: []
  # -- Environment variables to add to the read pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the read pods
  extraEnvFrom: []
  # -- Lifecycle for the read container
  lifecycle: {}
  # -- Volume mounts to add to the read pods
  extraVolumeMounts: []
  # -- Volumes to add to the read pods
  extraVolumes: []
  # -- Resource requests and limits for the read
  resources: {}
  # -- liveness probe settings for read pods. If empty, applies no livenessProbe
  livenessProbe: {}
    # -- statup probe for the read pods. If empty, applies no startupProbe
  startupProbe: {}
  # -- Grace period to allow the read to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Use the host's user namespace in the read pods.
  hostUsers: nil
  # -- Affinity for read pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: read
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- DNS config for read pods
  dnsConfig: {}
  # -- Node selector for read pods
  nodeSelector: {}
  # -- Topology Spread Constraints for read pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for read pods
  tolerations: []
  # -- The default is to deploy all pods in parallel.
  podManagementPolicy: "Parallel"
  # -- read.persistence is used only if legacyReadTarget is set to true
  persistence:
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: true
    # -- Set access modes on the PersistentVolumeClaim
    accessModes:
      - ReadWriteOnce
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Volume attributes class name to be used.
    # If empty or set to null, no volumeAttributesClassName spec is set.
    # Requires Kubernetes 1.31
    volumeAttributesClassName: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
    # -- Labels for volume claim
    labels: {}
# --  Configuration for the backend pod(s)
backend:
  # -- Number of replicas for the backend
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the backend.
    enabled: false
    # -- Minimum autoscaling replicas for the backend.
    minReplicas: 3
    # -- Maximum autoscaling replicas for the backend.
    maxReplicas: 6
    # -- Target CPU utilization percentage for the backend.
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilization percentage for the backend.
    targetMemoryUtilizationPercentage:
    # -- Behavior policies while scaling.
    behavior: {}
    #    scaleUp:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 60
    #    scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 180
  image:
    # -- The Docker registry for the backend image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the backend image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the backend image. Overrides `loki.image.tag`
    tag: null
  # -- The name of the PriorityClass for backend pods
  priorityClassName: null
  # -- Annotations for backend StatefulSet
  annotations: {}
  # -- Annotations for backend pods
  podAnnotations: {}
  # -- Additional labels for each `backend` pod
  podLabels: {}
  # -- Additional selector labels for each `backend` pod
  selectorLabels: {}
  service:
    # -- Annotations for backend Service
    annotations: {}
    # -- Additional labels for backend Service
    labels: {}
    # -- Service type for backend Service
    type: ClusterIP
    # -- trafficDistribution for backend Service
    trafficDistribution: ""
  # -- Comma-separated list of Loki modules to load for the backend
  targetModule: "backend"
  # -- Additional CLI args for the backend
  extraArgs: []
  # -- Environment variables to add to the backend pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the backend pods
  extraEnvFrom: []
  # -- Init containers to add to the backend pods
  initContainers: []
  # -- Containers to add to the backend pods
  extraContainers: []
  # -- Volume mounts to add to the backend pods
  extraVolumeMounts: []
  # -- Volumes to add to the backend pods
  extraVolumes: []
  # -- Resource requests and limits for the backend
  resources: {}
  # -- Grace period to allow the backend to shutdown before it is killed. Especially for the ingester,
  # this must be increased. It must be long enough so backends can be gracefully shutdown flushing/transferring
  # all data and to successfully leave the member ring on shutdown.
  terminationGracePeriodSeconds: 300
  # -- Use the host's user namespace in the backend pods.
  hostUsers: nil
  # -- Affinity for backend pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: backend
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- DNS config for backend pods
  dnsConfig: {}
  # -- Node selector for backend pods
  nodeSelector: {}
  # -- Topology Spread Constraints for backend pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for backend pods
  tolerations: []
  # -- The default is to deploy all pods in parallel.
  podManagementPolicy: "Parallel"
  persistence:
    # -- Enable volume claims in pod spec
    volumeClaimsEnabled: true
    # -- Set access modes on the PersistentVolumeClaim
    accessModes:
      - ReadWriteOnce
    # -- Parameters used for the `data` volume when volumeClaimEnabled if false
    dataVolumeParameters:
      emptyDir: {}
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: true
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Volume attributes class name to be used.
    # If empty or set to null, no volumeAttributesClassName spec is set.
    # Requires Kubernetes 1.31
    volumeAttributesClassName: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
    # -- Labels for volume claim
    labels: {}
######################################################################################################################
#
# Microservices Mode
#
# For large Loki deployments ingesting more than 1 TB/day
#
######################################################################################################################

# -- Configuration for the ingester
ingester:
  # -- Number of replicas for the ingester, when zoneAwareReplication.enabled is true, the total
  # number of replicas will match this value with each zone having 1/3rd of the total replicas.
  replicas: 0
  # -- DNSConfig for ingester pods
  dnsConfig: {}
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Use the host's user namespace in the ingester
  hostUsers: nil
  autoscaling:
    # -- Enable autoscaling for the ingester
    enabled: false
    # -- Minimum autoscaling replicas for the ingester
    minReplicas: 1
    # -- Maximum autoscaling replicas for the ingester
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the ingester
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the ingester
    targetMemoryUtilizationPercentage: null
    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
    customMetrics: []
    # - type: Pods
    #   pods:
    #     metric:
    #       name: loki_lines_total
    #     target:
    #       type: AverageValue
    #       averageValue: 10k
    behavior:
      # -- Enable autoscaling behaviours
      enabled: false
      # -- define scale down policies, must conform to HPAScalingRules
      scaleDown: {}
      # -- define scale up policies, must conform to HPAScalingRules
      scaleUp: {}
  image:
    # -- The Docker registry for the ingester image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the ingester image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `loki.image.tag`
    tag: null
  # -- Command to execute instead of defined in Docker image
  command: null
  labels: {}
  priorityClassName: null
  # -- Labels for ingester pods
  podLabels: {}
  # -- Annotations for ingester pods
  podAnnotations: {}
  # -- The name of the PriorityClass for ingester pods
  # -- Labels for ingester service
  serviceLabels: {}
  # -- Annotations for ingester service
  serviceAnnotations: {}
  # -- Service type for ingester service
  serviceType: "ClusterIP"
  # -- Additional CLI args for the ingester
  extraArgs: []
  # -- Environment variables to add to the ingester pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the ingester pods
  extraEnvFrom: []
  # -- Volume mounts to add to the ingester pods
  extraVolumeMounts: []
  # -- Volumes to add to the ingester pods
  extraVolumes: []
  # -- Resource requests and limits for the ingester
  resources: {}
  # -- Containers to add to the ingester pods
  extraContainers: []
  # -- Init containers to add to the ingester pods
  initContainers: []
  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,
  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring
  # all data and to successfully leave the member ring on shutdown.
  terminationGracePeriodSeconds: 300
  # -- Lifecycle for the ingester container
  lifecycle: {}
  # -- topologySpread for ingester pods.
  # @default -- Defaults to allow skew no more than 1 node
  # The value will be passed through tpl.
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/component: ingester
          app.kubernetes.io/name: '{{ include "loki.name" . }}'
          app.kubernetes.io/instance: '{{ .Release.Name }}'
  # -- Affinity for ingester pods. Ignored if zoneAwareReplication is enabled.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: ingester
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: 1
  # -- Node selector for ingester pods
  nodeSelector: {}
  # -- Tolerations for ingester pods
  tolerations: []
  # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
  readinessProbe: {}
  # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
  livenessProbe: {}
  # -- startup probe settings for ingester pods. If empty use `loki.startupProbe`
  startupProbe: {}
  # -- UpdateStrategy for the ingester StatefulSets.
  updateStrategy:
    # -- One of  'OnDelete' or 'RollingUpdate'
    type: RollingUpdate
    # -- Optional for updateStrategy.type=RollingUpdate. See [Partitioned rolling updates](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions) in the StatefulSet docs for details.
    # rollingUpdate:
    #   partition: 0
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: false
    # -- Use emptyDir with ramdisk for storage. **Please note that all data in ingester will be lost on pod restart**
    inMemory: false
    # -- List of the ingester PVCs
    # @notationType -- list
    claims:
      - name: data
        # -- Set access modes on the PersistentVolumeClaim
        accessModes:
          - ReadWriteOnce
        size: 10Gi
        #   -- Storage class to be used.
        #   If defined, storageClassName: <storageClass>.
        #   If set to "-", storageClassName: "", which disables dynamic provisioning.
        #   If empty or set to null, no storageClassName spec is
        #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
        storageClass: null
        # -- Volume attributes class name to be used.
        # If empty or set to null, no volumeAttributesClassName spec is set.
        # Requires Kubernetes 1.31
        volumeAttributesClassName: null
        # - name: wal
        #   size: 150Gi
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: false
    whenDeleted: Retain
    whenScaled: Retain
  # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: ""
  # -- trafficDistribution for ingester service
  trafficDistribution: ""
  # -- Enabling zone awareness on ingesters will create 3 statefulests where all writes will send a replica to each zone.
  # This is primarily intended to accelerate rollout operations by allowing for multiple ingesters within a single
  # zone to be shutdown and restart simultaneously (the remaining 2 zones will be guaranteed to have at least one copy
  # of the data).
  # Note: This can be used to run Loki over multiple cloud provider availability zones however this is not currently
  # recommended as Loki is not optimized for this and cross zone network traffic costs can become extremely high
  # extremely quickly. Even with zone awareness enabled, it is recommended to run Loki in a single availability zone.
  zoneAwareReplication:
    # -- Enable zone awareness.
    enabled: true
    # -- The percent of replicas in each zone that will be restarted at once. In a value of 0-100
    maxUnavailablePct: 33
    # -- zoneA configuration
    zoneA:
      # -- optionally define a node selector for this zone
      nodeSelector: null
      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
      # The value will be passed through tpl.
      extraAffinity: {}
      # -- Specific annotations to add to zone A statefulset
      annotations: {}
      # -- Specific annotations to add to zone A pods
      podAnnotations: {}
    zoneB:
      # -- optionally define a node selector for this zone
      nodeSelector: null
      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
      # The value will be passed through tpl.
      extraAffinity: {}
      # -- Specific annotations to add to zone B statefulset
      annotations: {}
      # -- Specific annotations to add to zone B pods
      podAnnotations: {}
    zoneC:
      # -- optionally define a node selector for this zone
      nodeSelector: null
      # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
      # The value will be passed through tpl.
      extraAffinity: {}
      # -- Specific annotations to add to zone C statefulset
      annotations: {}
      # -- Specific annotations to add to zone C pods
      podAnnotations: {}
    # -- The migration block allows migrating non zone aware ingesters to zone aware ingesters.
    migration:
      enabled: false
      excludeDefaultZone: false
      readPath: false
      writePath: false

  # optionally allow adding arbitrary prefix to the ingester rollout-group label
  rolloutGroupPrefix: null
  # optionally allow adding 'loki-' prefix to ingester name label
  addIngesterNamePrefix: false

# --  Configuration for the distributor
distributor:
  # -- Number of replicas for the distributor
  replicas: 0
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Use the host's user namespace in the distributor
  hostUsers: nil
  # -- DNSConfig for distributor pods
  dnsConfig: {}
  autoscaling:
    # -- Enable autoscaling for the distributor
    enabled: false
    # -- Minimum autoscaling replicas for the distributor
    minReplicas: 1
    # -- Maximum autoscaling replicas for the distributor
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the distributor
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the distributor
    targetMemoryUtilizationPercentage: null
    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
    customMetrics: []
    # - type: Pods
    #   pods:
    #     metric:
    #       name: loki_lines_total
    #     target:
    #       type: AverageValue
    #       averageValue: 10k
    behavior:
      # -- Enable autoscaling behaviours
      enabled: false
      # -- define scale down policies, must conform to HPAScalingRules
      scaleDown: {}
      # -- define scale up policies, must conform to HPAScalingRules
      scaleUp: {}
  image:
    # -- The Docker registry for the distributor image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the distributor image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the distributor image. Overrides `loki.image.tag`
    tag: null
  # -- Command to execute instead of defined in Docker image
  command: null
  # -- The name of the PriorityClass for distributor pods
  priorityClassName: null
  # -- Labels for distributor pods
  podLabels: {}
  # -- Annotations for distributor pods
  podAnnotations: {}
  # -- Labels for distributor service
  serviceLabels: {}
  # -- Annotations for distributor service
  serviceAnnotations: {}
  # -- Service type for distributor service
  serviceType: ClusterIP
  # -- Additional CLI args for the distributor
  extraArgs: []
  # -- Environment variables to add to the distributor pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the distributor pods
  extraEnvFrom: []
  # -- Volume mounts to add to the distributor pods
  extraVolumeMounts: []
  # -- Volumes to add to the distributor pods
  extraVolumes: []
  # -- Resource requests and limits for the distributor
  resources: {}
  # -- Init containers to add to the distributor pods
  initContainers: []
  # -- Containers to add to the distributor pods
  extraContainers: []
  # -- Grace period to allow the distributor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for distributor pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: distributor
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: null
  # -- Max Surge for distributor pods
  maxSurge: 0
  # -- Node selector for distributor pods
  nodeSelector: {}
  # -- Topology Spread Constraints for distributor pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for distributor pods
  tolerations: []
  # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: ""
  # -- trafficDistribution for distributor service
  trafficDistribution: ""
# --  Configuration for the querier
querier:
  # -- Number of replicas for the querier
  replicas: 0
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Use the host's user namespace in the querier
  hostUsers: nil
  autoscaling:
    # -- Enable autoscaling for the querier, this is only used if `indexGateway.enabled: true`
    enabled: false
    # -- Minimum autoscaling replicas for the querier
    minReplicas: 1
    # -- Maximum autoscaling replicas for the querier
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the querier
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the querier
    targetMemoryUtilizationPercentage: null
    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
    customMetrics: []
    # - type: External
    #   external:
    #     metric:
    #       name: loki_inflight_queries
    #     target:
    #       type: AverageValue
    #       averageValue: 12
    behavior:
      # -- Enable autoscaling behaviours
      enabled: false
      # -- define scale down policies, must conform to HPAScalingRules
      scaleDown: {}
      # -- define scale up policies, must conform to HPAScalingRules
      scaleUp: {}
  image:
    # -- The Docker registry for the querier image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the querier image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the querier image. Overrides `loki.image.tag`
    tag: null
  # -- Command to execute instead of defined in Docker image
  command: null
  # -- The name of the PriorityClass for querier pods
  priorityClassName: null
  # -- Labels for querier pods
  podLabels: {}
  # -- Annotations for querier pods
  podAnnotations: {}
  # -- Labels for querier service
  serviceLabels: {}
  # -- Annotations for querier service
  serviceAnnotations: {}
  # -- Service Type for querier service
  serviceType: "ClusterIP"
  # -- Additional CLI args for the querier
  extraArgs: []
  # -- Environment variables to add to the querier pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the querier pods
  extraEnvFrom: []
  # -- Volume mounts to add to the querier pods
  extraVolumeMounts: []
  # -- Volumes to add to the querier pods
  extraVolumes: []
  # -- Resource requests and limits for the querier
  resources: {}
  # -- Containers to add to the querier pods
  extraContainers: []
  # -- Init containers to add to the querier pods
  initContainers: []
  # -- Grace period to allow the querier to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for querier pods.
  # @default -- Defaults to allow skew no more then 1 node
  # The value will be passed through tpl.
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/component: querier
          app.kubernetes.io/name: '{{ include "loki.name" . }}'
          app.kubernetes.io/instance: '{{ .Release.Name }}'
  # -- Affinity for querier pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: querier
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: null
  # -- Max Surge for querier pods
  maxSurge: 0
  # -- Node selector for querier pods
  nodeSelector: {}
  # -- Tolerations for querier pods
  tolerations: []
  # -- DNSConfig for querier pods
  dnsConfig: {}
  # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: ""
  # -- trafficDistribution for querier service
  trafficDistribution: ""
# -- Configuration for the query-frontend
queryFrontend:
  # -- Number of replicas for the query-frontend
  replicas: 0
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Use the host's user namespace in the query-frontend
  hostUsers: nil
  autoscaling:
    # -- Enable autoscaling for the query-frontend
    enabled: false
    # -- Minimum autoscaling replicas for the query-frontend
    minReplicas: 1
    # -- Maximum autoscaling replicas for the query-frontend
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the query-frontend
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the query-frontend
    targetMemoryUtilizationPercentage: null
    # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
    customMetrics: []
    # - type: Pods
    #   pods:
    #     metric:
    #       name: loki_query_rate
    #     target:
    #       type: AverageValue
    #       averageValue: 100
    behavior:
      # -- Enable autoscaling behaviours
      enabled: false
      # -- define scale down policies, must conform to HPAScalingRules
      scaleDown: {}
      # -- define scale up policies, must conform to HPAScalingRules
      scaleUp: {}
  image:
    # -- The Docker registry for the query-frontend image. Overrides `loki.image.registry`
    registry: null
    # -- Docker image repository for the query-frontend image. Overrides `loki.image.repository`
    repository: null
    # -- Docker image tag for the query-frontend image. Overrides `loki.image.tag`
    tag: null
  # -- Command to execute instead of defined in Docker image
  command: null
  # -- The name of the PriorityClass for query-frontend pods
  priorityClassName: null
  # -- Labels for query-frontend pods
  podLabels: {}
  # -- Annotations for query-frontend pods
  podAnnotations: {}
  # -- Labels for query-frontend service
  serviceLabels: {}
  # -- Annotations for query-frontend service
  serviceAnnotations: {}
  # -- Service Type for query-frontend service
  serviceType: ClusterIP
  # -- Additional CLI args for the query-frontend
  extraArgs: []
  # -- Environment variables to add to the query-frontend pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the query-frontend pods
  extraEnvFrom: []
  # -- Volume mounts to add to the query-frontend pods
  extraVolumeMounts: []
  # -- Volumes to add to the query-frontend pods
  extraVolumes: []
  # -- Resource requests and limits for the query-frontend
  resources: {}
  # -- init containers to add to the query-frontend pods
  initContainers: []
  # -- Containers to add to the query-frontend pods
  extraContainers: []
  # -- Grace period to allow the query-frontend to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for query-frontend pods.
  # @default -- Hard node anti-affinity
  # The value will be passed through tpl.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: query-frontend
              app.kubernetes.io/name: '{{ include "loki.name" . }}'
              app.kubernetes.io/instance: '{{ .Release.Name }}'
          topologyKey: kubernetes.io/hostname
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: null
  # -- Node selector for query-frontend pods
  nodeSelector: {}
  # -- Topology Spread Constraints for query-frontend pods
  # The value will be passed through tpl.
  topologySpreadConstraints: []
  # -- Tolerations for query-frontend pods
  tolerations: []
  # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: ""
  # -- Enable load balancer port for query-frontend
  loadBalancer:
    enabled: true
  # -- trafficDistribution for query-frontend service
  trafficDistribution: ""

# You can use a self hosted memcached by setting enabled to false and providing addresses.
memcached:
  # -- Enable the built in memcached server provided by the chart
  enabled: true
  image:
    # -- Memcached Docker image repository
    repository: memcached
    # -- Memcached Docker image tag
    tag: 1.6.39-alpine
    # -- Memcached Docker image pull policy
    pullPolicy: IfNotPresent
  # -- The SecurityContext override for memcached pods
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 11211
    runAsGroup: 11211
    fsGroup: 11211
  # -- The name of the PriorityClass for memcached pods
  priorityClassName: null
  # -- The SecurityContext for memcached containers
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]
    allowPrivilegeEscalation: false
  # -- Readiness probe for memcached pods (probe port defaults to container port)
  readinessProbe:
    tcpSocket:
      port: client
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 6
  # -- Liveness probe for memcached pods
  livenessProbe:
    tcpSocket:
      port: client
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  # -- Startup probe for memcached pods
  startupProbe: {}

memcachedExporter:
  # -- Whether memcached metrics should be exported
  enabled: true
  image:
    repository: prom/memcached-exporter
    tag: v0.15.4
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    limits: {}
  # -- The SecurityContext for memcached exporter containers
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]
    allowPrivilegeEscalation: false
  # -- Extra args to add to the exporter container.
  # Example:
  # extraArgs:
  #   memcached.tls.enable: true
  #   memcached.tls.cert-file: /certs/cert.crt
  #   memcached.tls.key-file: /certs/cert.key
  #   memcached.tls.ca-file: /certs/ca.crt
  #   memcached.tls.insecure-skip-verify: false
  #   memcached.tls.server-name: memcached
  extraArgs: {}
  # -- Liveness probe for memcached exporter
  livenessProbe:
    httpGet:
      path: /metrics
      port: http-metrics
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  # -- Readiness probe for memcached exporter
  readinessProbe:
    httpGet:
      path: /metrics
      port: http-metrics
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  # -- Startup probe for memcached exporter
  startupProbe: {}

resultsCache:
  # -- Specifies whether memcached based results-cache should be enabled
  enabled: true
  # -- Comma separated addresses list in DNS Service Discovery format
  addresses: dnssrvnoa+_memcached-client._tcp.{{ include "loki.resourceName" (dict "ctx" $ "component" "results-cache") }}.{{ include "loki.namespace" $ }}.svc.{{ .Values.global.clusterDomain }}
  # -- Specify how long cached results should be stored in the results-cache before being expired
  defaultValidity: 12h
  # -- Memcached operation timeout
  timeout: 500ms
  # -- Total number of results-cache replicas
  replicas: 1
  # -- Port of the results-cache service
  port: 11211
  # -- Amount of memory allocated to results-cache for object storage (in MB).
  allocatedMemory: 1024
  # -- Maximum item results-cache for memcached (in MB).
  maxItemMemory: 5
  # -- Maximum number of connections allowed
  connectionLimit: 16384
  # -- Max memory to use for cache write back
  writebackSizeLimit: 500MB
  # -- Max number of objects to use for cache write back
  writebackBuffer: 500000
  # -- Number of parallel threads for cache write back
  writebackParallelism: 1
  # -- Extra init containers for results-cache pods
  initContainers: []
  # -- Annotations for the results-cache pods
  annotations: {}
  # -- Node selector for results-cache pods
  nodeSelector: {}
  # -- Affinity for results-cache pods
  affinity: {}
  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: []
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway
  # -- Tolerations for results-cache pods
  tolerations: []
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: 1
  # -- DNSConfig for results-cache
  dnsConfig: {}
  # -- The name of the PriorityClass for results-cache pods
  priorityClassName: null
  # -- Use the host's user namespace in results-cache pods
  hostUsers: nil
  # -- Labels for results-cache pods
  podLabels: {}
  # -- Annotations for results-cache pods
  podAnnotations: {}
  # -- Management policy for results-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the results-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60
  # -- Stateful results-cache strategy
  statefulStrategy:
    type: RollingUpdate
  # -- Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,modern,track_sizes'
  extraExtendedOptions: ""
  # -- Additional CLI args for results-cache
  extraArgs: {}
  # -- Additional containers to be added to the results-cache pod.
  extraContainers: []
  # -- Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []
  # -- Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []
  # -- Resource requests and limits for the results-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null
  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}
  # -- Persistence settings for the results-cache
  persistence:
    # -- Enable creating PVCs for the results-cache
    enabled: false
    # -- Size of persistent disk, must be in G or Gi
    storageSize: 10G
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Volume attributes class name to be used.
    # If empty or set to null, no volumeAttributesClassName spec is set.
    # Requires Kubernetes 1.31
    volumeAttributesClassName: null
    # -- Volume mount path
    mountPath: /data
    # -- PVC additional labels
    labels: {}
chunksCache:
  # -- Append to the name of the resources to make names different for l1 and l2
  suffix: ""
  # -- Specifies whether memcached based chunks-cache should be enabled
  enabled: true
  # -- Comma separated addresses list in DNS Service Discovery format
  addresses: 'dnssrvnoa+_memcached-client._tcp.{{ include "loki.resourceName" (dict "ctx" $ "component" "chunks-cache" "suffix" $.Values.chunksCache.suffix ) }}.{{ include "loki.namespace" $ }}.svc.{{ .Values.global.clusterDomain }}'
  # -- Batchsize for sending and receiving chunks from chunks cache
  batchSize: 4
  # -- Parallel threads for sending and receiving chunks from chunks cache
  parallelism: 5
  # -- Memcached operation timeout
  timeout: 2000ms
  # -- Specify how long cached chunks should be stored in the chunks-cache before being expired
  defaultValidity: 0s
  # -- Specify how long cached chunks should be stored in the chunks-cache before being expired
  replicas: 1
  # -- Port of the chunks-cache service
  port: 11211
  # -- Amount of memory allocated to chunks-cache for object storage (in MB).
  allocatedMemory: 8192
  # -- Maximum item memory for chunks-cache (in MB).
  maxItemMemory: 5
  # -- Maximum number of connections allowed
  connectionLimit: 16384
  # -- Max memory to use for cache write back
  writebackSizeLimit: 500MB
  # -- Max number of objects to use for cache write back
  writebackBuffer: 500000
  # -- Number of parallel threads for cache write back
  writebackParallelism: 1
  # -- Extra init containers for chunks-cache pods
  initContainers: []
  # -- Annotations for the chunks-cache pods
  annotations: {}
  # -- Node selector for chunks-cache pods
  nodeSelector: {}
  # -- Affinity for chunks-cache pods
  affinity: {}
  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: []
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway
  # -- Tolerations for chunks-cache pods
  tolerations: []
  # -- Pod Disruption Budget maxUnavailable
  maxUnavailable: 1
  # -- DNSConfig for chunks-cache
  dnsConfig: {}
  # -- The name of the PriorityClass for chunks-cache pods
  priorityClassName: null
  # -- Use the host's user namespace in chunks-cache pods
  hostUsers: nil
  # -- Labels for chunks-cache pods
  podLabels: {}
  # -- Annotations for chunks-cache pods
  podAnnotations: {}
  # -- Management policy for chunks-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the chunks-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60
  # -- Stateful chunks-cache strategy
  statefulStrategy:
    type: RollingUpdate
  # -- Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag.
  # Example:
  # extraExtendedOptions: 'tls,no_hashexpand'
  extraExtendedOptions: ""
  # -- Additional CLI args for chunks-cache
  extraArgs: {}
  # -- Additional containers to be added to the chunks-cache pod.
  extraContainers: []
  # -- Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumes:
  # - name: extra-volume
  #   secret:
  #    secretName: extra-volume-secret
  extraVolumes: []
  # -- Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers).
  # Example:
  # extraVolumeMounts:
  # - name: extra-volume
  #   mountPath: /etc/extra-volume
  #   readOnly: true
  extraVolumeMounts: []
  # -- Resource requests and limits for the chunks-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null
  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}
  # -- Persistence settings for the chunks-cache
  persistence:
    # -- Enable creating PVCs for the chunks-cache
    enabled: false
    # -- Size of persistent disk, must be in G or Gi
    storageSize: 10G
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Volume attributes class name to be used.
    # If empty or set to null, no volumeAttributesClassName spec is set.
    # Requires Kubernetes 1.31
    volumeAttributesClassName: null
    # -- Volume mount path
    mountPath: /data
    labels: {}
  # -- l2 memcache configuration
  l2:
    # -- Append to the name of the resources to make names different for l1 and l2
    suffix: "l2"
    # -- The age of chunks should be transfered from l1 cache to l2
    # 4 days
    l2ChunkCacheHandoff: 345600s
    # -- Specifies whether memcached based chunks-cache-l2 should be enabled
    enabled: false
    # -- Comma separated addresses list in DNS Service Discovery format
    addresses: 'dnssrvnoa+_memcached-client._tcp.{{ include "loki.resourceName" (dict "ctx" $ "component" "chunks-cache" "suffix" $.Values.chunksCache.l2.suffix ) }}.{{ include "loki.namespace" $ }}.svc.{{ .Values.global.clusterDomain }}'
    # -- Batchsize for sending and receiving chunks from chunks cache
    batchSize: 4
    # -- Parallel threads for sending and receiving chunks from chunks cache
    parallelism: 5
    # -- Memcached operation timeout
    timeout: 2000ms
    # -- Specify how long cached chunks should be stored in the chunks-cache-l2 before being expired
    defaultValidity: 0s
    # -- Specify how long cached chunks should be stored in the chunks-cache-l2 before being expired
    replicas: 1
    # -- Port of the chunks-cache-l2 service
    port: 11211
    # -- Amount of memory allocated to chunks-cache-l2 for object storage (in MB).
    allocatedMemory: 8192
    # -- Maximum item memory for chunks-cache-l2 (in MB).
    maxItemMemory: 5
    # -- Maximum number of connections allowed
    connectionLimit: 16384
    # -- Max memory to use for cache write back
    writebackSizeLimit: 500MB
    # -- Max number of objects to use for cache write back
    writebackBuffer: 500000
    # -- Number of parallel threads for cache write back
    writebackParallelism: 1
    # -- Extra init containers for chunks-cache-l2 pods
    initContainers: []
    # -- Annotations for the chunks-cache-l2 pods
    annotations: {}
    # -- Node selector for chunks-cach-l2 pods
    nodeSelector: {}
    # -- Affinity for chunks-cache-l2 pods
    affinity: {}
    # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
    # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
    topologySpreadConstraints: []
    #  maxSkew: 1
    #  topologyKey: kubernetes.io/hostname
    #  whenUnsatisfiable: ScheduleAnyway
    # -- Tolerations for chunks-cache-l2 pods
    tolerations: []
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- DNSConfig for chunks-cache-l2
    dnsConfig: {}
    # -- The name of the PriorityClass for chunks-cache-l2 pods
    priorityClassName: null
    # -- Use the host's user namespace in chunks-cache-l2 pods
    hostUsers: nil
    # -- Labels for chunks-cache-l2 pods
    podLabels: {}
    # -- Annotations for chunks-cache-l2 pods
    podAnnotations: {}
    # -- Management policy for chunks-cache-l2 pods
    podManagementPolicy: Parallel
    # -- Grace period to allow the chunks-cache-l2 to shutdown before it is killed
    terminationGracePeriodSeconds: 60
    # -- Stateful chunks-cache strategy
    statefulStrategy:
      type: RollingUpdate
    # -- Add extended options for chunks-cache-l2 memcached container. The format is the same as for the memcached -o/--extend flag.
    # Example:
    # extraExtendedOptions: 'tls,no_hashexpand'
    extraExtendedOptions: ""
    # -- Additional CLI args for chunks-cache-l2
    extraArgs: {}
    # -- Additional containers to be added to the chunks-cache-l2 pod.
    extraContainers: []
    # -- Additional volumes to be added to the chunks-cache-l2 pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumes:
    # - name: extra-volume
    #   secret:
    #    secretName: extra-volume-secret
    extraVolumes: []
    # -- Additional volume mounts to be added to the chunks-cache-l2 pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumeMounts:
    # - name: extra-volume
    #   mountPath: /etc/extra-volume
    #   readOnly: true
    extraVolumeMounts: []
    # -- Resource requests and limits for the chunks-cache-l2
    # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
    resources: null
    # -- Service annotations and labels
    service:
      annotations: {}
      labels: {}
    # -- Persistence settings for the chunks-cache-l2
    persistence:
      # -- Enable creating PVCs for the chunks-cache-l2
      enabled: false
      # -- Size of persistent disk, must be in G or Gi
      storageSize: 10G
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Volume attributes class name to be used.
      # If empty or set to null, no volumeAttributesClassName spec is set.
      # Requires Kubernetes 1.31
      volumeAttributesClassName: null
      # -- Volume mount path
      mountPath: /data
      labels: {}
######################################################################################################################
#
# Subchart configurations
#
######################################################################################################################
# -- Setting for the Grafana Rollout Operator https://github.com/grafana/helm-charts/tree/main/charts/rollout-operator
rollout_operator:
  enabled: false
  # -- podSecurityContext is the pod security context for the rollout operator.
  # When installing on OpenShift, override podSecurityContext settings with
  #
  # rollout_operator:
  #   podSecurityContext:
  #     fsGroup: null
  #     runAsGroup: null
  #     runAsUser: null
  podSecurityContext:
    fsGroup: 10001
    runAsGroup: 10001
    runAsNonRoot: true
    runAsUser: 10001
    seccompProfile:
      type: RuntimeDefault
  # Set the container security context
  securityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]
    allowPrivilegeEscalation: false

# Create extra manifests via values
# Can be a list or dictionary, both are passed through `tpl`.  If dict, keys are ignored and only values are used.
# Objects can also be defined as multiline strings, useful for templating field names
extraObjects: null
# - apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: loki-alerting-rules
#   data:
#     loki-alerting-rules.yaml: |-
#       groups:
#         - name: example
#           rules:
#           - alert: example
#             expr: |
#               sum(count_over_time({app="loki"} |~ "error")) > 0
#             for: 3m
#             labels:
#               severity: warning
#               category: logs
#             annotations:
#               message: "loki has encountered errors"
# - |
#     apiVersion: v1
#     kind: Secret
#     type: Opaque
#     metadata:
#       name: loki-distributed-basic-auth
#     data:
#       {{- range .Values.loki.tenants }}
#       {{ .name }}: {{ b64enc .password | quote }}
#       {{- end }}

# -- Monitoring section determines which monitoring features to enable
monitoring:
  # Dashboards for monitoring Loki
  dashboards:
    # -- If enabled, create configmap with dashboards for monitoring Loki
    enabled: true
    # -- Alternative namespace to create dashboards ConfigMap in
    namespace: kube-prometheus-stack
    # -- Additional annotations for the dashboards ConfigMap
    annotations: {}
    # -- Labels for the dashboards ConfigMap
    labels:
      grafana_dashboard: "1"
  # -- Recording rules for monitoring Loki, required for some dashboards
  rules:
    # -- If enabled, create PrometheusRule resource with Loki recording rules
    enabled: false
    # -- Include alerting rules
    alerting: true
    # -- Specify which individual alerts should be disabled
    # -- Instead of turning off each alert one by one, set the .monitoring.rules.alerting value to false instead.
    # -- If you disable all the alerts and keep .monitoring.rules.alerting set to true, the chart will fail to render.
    #
    # -- DEPRECATED: use monitoring.rules.configs.*.enabled instead
    disabled: {}
    #  LokiRequestErrors: true
    #  LokiRequestPanics: true

    configs:
      LokiRequestErrors:
        enabled: true
        for: 15m
        lookbackPeriod: 2m
        severity: critical
        threshold: 10
      LokiRequestPanics:
        enabled: true
        lookbackPeriod: 10m
        severity: critical
        threshold: 0
      LokiRequestLatency:
        enabled: true
        for: 15m
        severity: critical
        threshold: 1
      LokiTooManyCompactorsRunning:
        enabled: true
        for: 5m
        severity: warning
      LokiCanaryLatency:
        enabled: true
        for: 15m
        lookbackPeriod: 5m
        severity: warning
        threshold: 5

    # -- Alternative namespace to create PrometheusRule resources in
    namespace: null
    # -- Additional annotations for the rules PrometheusRule resource
    annotations: {}
    # -- Additional labels for the rules PrometheusRule resource
    labels: {}
    # -- Additional annotations for PrometheusRule alerts
    additionalRuleAnnotations: {}
    # e.g.:
    # additionalRuleAnnotations:
    #   runbook_url: "https://runbooks.example.com/oncall/loki"
    #   summary: "What this alert means and how to respond"
    # -- Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}
    # -- Additional groups to add to the rules file
    additionalGroups: []
    # - name: additional-loki-rules
    #   rules:
    #     - record: job:loki_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)
